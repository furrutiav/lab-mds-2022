{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e96b59b",
   "metadata": {
    "cell_id": "00006-84a35c5d-0758-4cbb-b2ca-b182898b80d0",
    "deepnote_cell_height": 295.8833312988281,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "\n",
    "\n",
    "# Proyecto\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- Sebastian Avendaño\n",
    "- Felipe Urrutia\n",
    "\n",
    "- \\<Nombre de usuarios en Codalab\\>\n",
    "\n",
    "- \\<Nombre del Equipo en Codalab\\>\n",
    "\n",
    "### Link de repositorio de GitHub: https://github.com/furrutiav/lab-mds-2022\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba8db88-52a2-47d9-8d4f-71e4fe81f833",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Librerías Utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c27824f-57d0-4eea-ab8e-1db9ad9fde73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\felip\\anaconda3\\lib\\site-packages (8.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\felip\\anaconda3\\lib\\site-packages (from pyarrow) (1.20.3)\n"
     ]
    }
   ],
   "source": [
    "# Carga y Preparación de los datos\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install pyarrow\n",
    "\n",
    "# EDA\n",
    "import plotly.express as px\n",
    "\n",
    "# 3. Preprocess\n",
    "## Hold out\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Preprocess\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize  \n",
    "from nltk.stem import PorterStemmer\n",
    "## ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "## FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# 4. Clasificacion\n",
    "## Dummy y Baseline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "## Modelos\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2, SelectKBest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "## Gridsearch\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, GridSearchCV\n",
    "# Prediccion\n",
    "from zipfile import ZipFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975667a2",
   "metadata": {
    "cell_id": "00008-6607fdcd-a35f-4e75-9b46-da3b181c1551",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "## 2. Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc572b58",
   "metadata": {},
   "source": [
    "#### Carga y Preparación de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7863e",
   "metadata": {},
   "source": [
    "<!-- - Cargar los datos con Pandas y fusionar por `id`.\n",
    "- Eliminar columnas `'poster_path'`, `'backdrop_path'`, `'recommendations'`.\n",
    "- Filtrar ejemplos con `revenue` igual a 0.\n",
    "- Filtrar ejemplos con `release_date` y `runtime` nulos.\n",
    "- Convertir fechas de release_date a `pd.DateTime`.\n",
    "- Conservar solo los ejemplos con `status` `\"Released\"`.\n",
    "- Rellenar valores nulos categóricos y de texto con `''`.\n",
    "- Discretizar `vote_average` a los siguientes bins y guardar los resultados en la columna `label`: \n",
    "  - (0, 5]: `'Negative'`\n",
    "  - (5, 6]: `'Mixed'`\n",
    "  - (6, 7]: `'Mostly Positive'`\n",
    "  - (7, 8]: `'Positive'`\n",
    "  - (8, 10]: `'Very Positive'`\n",
    "- Eliminar la columna `vote_average` e `id`\n",
    "- Renombrar la columna `revenue` por `target`. -->\n",
    "En esta subseccion realizaremos la carga y preparacion de los datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161be2a",
   "metadata": {},
   "source": [
    "**Cargar los datos con Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9dd483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atributos numericos\n",
    "train_numerical_features = pd.read_parquet(\n",
    "    'train_numerical_features.parquet'\n",
    ").set_index(\"id\")\n",
    "# Atributos textuales\n",
    "train_text_features = pd.read_parquet(\n",
    "    'train_text_features.parquet'\n",
    ").set_index(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f1cec",
   "metadata": {},
   "source": [
    "**Fusionar por id:** Fusionamos ambos conjuntos de datos sobre la llave $\\texttt{id}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d257939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_numerical_features, train_text_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202cca5",
   "metadata": {},
   "source": [
    "**Eliminar columnas duplicadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ef5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T.drop_duplicates().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe140d7",
   "metadata": {},
   "source": [
    "**Eliminar columnas $\\texttt{poster_path}$, $\\texttt{backdrop_path}$, $\\texttt{recommendations}$:** Columnas que no son utiles para nuestro problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c150a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['poster_path', 'backdrop_path', 'recommendations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4fd31c",
   "metadata": {},
   "source": [
    "**Filtrar ejemplos con valor de la variable $\\texttt{revenue}$ igual a 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f2630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"revenue\"]>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584fa989",
   "metadata": {},
   "source": [
    "**Filtrar ejemplos con $\\texttt{release_date}$ y $\\texttt{runtime}$ nulos** (Nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\texttt{release_date}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a26c07ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"release_date\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\texttt{runtime}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f473d059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"runtime\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7873aba",
   "metadata": {},
   "source": [
    "**Convertir fechas de $\\texttt{release_date}$ a $\\texttt{pd.DateTime}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecde65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"release_date\"] = df[\"release_date\"].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad39d9",
   "metadata": {},
   "source": [
    "**Conservar solo los ejemplos con valor de $\\texttt{status}$ igual a $\\texttt{Released}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f12ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"status\"] == \"Released\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fdfaf7",
   "metadata": {},
   "source": [
    "**Rellenar valores nulos categóricos y de texto con ' '**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05dd5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[train_text_features.columns.tolist()] = df[train_text_features.columns.tolist()].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40fb62e",
   "metadata": {},
   "source": [
    "**Discretizar vote_average a los siguientes bins y guardar los resultados en la columna label:**\n",
    "\n",
    "    (0, 5]: 'Negative'\n",
    "    (5, 6]: 'Mixed'\n",
    "    (6, 7]: 'Mostly Positive'\n",
    "    (7, 8]: 'Positive'\n",
    "    (8, 10]: 'Very Positive'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10b7c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = pd.cut(df['vote_average'], \n",
    "       bins=[0,5,6,7,8,10], \n",
    "       labels=[\"Negative\", \"Mixed\", \"Mostly Positive\", \"Positive\", \"Very Positive\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b637862",
   "metadata": {},
   "source": [
    "**Eliminar la columna $\\texttt{vote_average}$ e $\\texttt{id}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50789594",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"vote_average\")\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ea6b5",
   "metadata": {},
   "source": [
    "**Renombrar la columna $\\texttt{revenue}$ por $\\texttt{target}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44993017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"revenue\": \"target\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892709c2",
   "metadata": {},
   "source": [
    "**Cargamos los clusters obtenidos con KMeans y BERT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos datos son un conjunto de las siguientes tres variables:\n",
    "* $\\texttt{clusters_keywords}$\n",
    "* $\\texttt{clusters_overview}$\n",
    "* $\\texttt{clusters_tagline}$\n",
    "\n",
    "Cada una identifica el cluster al que pertenece la pelicula cuando se obtiene el sentence embedding usando BERT del texto de $\\texttt{keywords}$, $\\texttt{overview}$ y $\\texttt{tagline}$, según corresponda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_BERT = pickle.load(open(\"clusters_BERT.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos estas nuevas variables con el dataframe principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eafd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(clusters_BERT, how=\"left\", on=\"title\")\n",
    "df[\"clusters_keywords clusters_overview clusters_tagline\".split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa072c",
   "metadata": {
    "cell_id": "00011-95957584-71f1-4669-a6e5-9b8ac7b8d4e0",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Preprocesamiento, Holdout y Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e450ed7",
   "metadata": {},
   "source": [
    "#### ColumnTransformer y Holdout\n",
    "\n",
    "<!-- *Esta sección consiste en generar los distintos pasos para preparar sus datos con el fin de luego poder crear su modelo.*\n",
    "\n",
    "Generar un ColumnTransformer que:\n",
    "\n",
    "- Preprocese datos categóricos y ordinales.\n",
    "- Escale/estandarice datos numéricos.\n",
    "- Codifique texto.\n",
    "\n",
    "Luego, pruebe las transformaciones utilizando `fit_transform` y `get_feature_names out`.\n",
    "\n",
    "Posteriormente, ejecute un Holdout que le permita más adelante evaluar los modelos. **Recuerde eliminar los target y las labels del dataset antes de dividirlo**. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holdout train/test para cada problema de predicción**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos los conjuntos de datos para cada problema, con sus respectivas variables a predecir y omitiendo en cada conjunto ambas variables a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b38b7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clf, y_clf = df.drop(columns=[\"target\", \"label\"]), df[\"label\"]\n",
    "X_reg, y_reg = df.drop(columns=[\"target\", \"label\"]), df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split de los datos para cada problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91249544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificacion\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(\n",
    "    X_clf, \n",
    "    y_clf, \n",
    "    test_size=0.20, \n",
    "    random_state=23\n",
    ")\n",
    "# Regresion\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, \n",
    "    y_reg, \n",
    "    test_size=0.20,\n",
    "    random_state=23\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ColumnTransformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codigo de ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda que sigue se mencionan aquellas variables numericas que pasaran por una transformación minmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos_minmax = [\n",
    "    \"budget\",\n",
    "    \"release_date_month\",\n",
    "    \"release_date_day\",\n",
    "    \"num_top_production_companies\",\n",
    "    \"num_top_artists\",\n",
    "    \"ratio(runtime, budget)\",\n",
    "    \"num_artists\",\n",
    "    \"num_genres\",\n",
    "    \"num_production_companies\",\n",
    "    \"prop(num_top_production_companies)\",\n",
    "    \"prop(num_top_artists)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda que sigue se mencionan aquellas variables numericas que pasaran por una transformación *standard*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos_st = [\n",
    "    \"runtime\",\n",
    "    \"release_date_timestamp\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda que sigue se mencionan aquellas variables categoricas que pasaran por una vectorizacion OneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos_onehot = [\n",
    "    \"original_language\",\n",
    "    \"clusters_keywords\",\n",
    "    \"clusters_overview\",\n",
    "    \"clusters_tagline\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideramos dos tipos de tokenizadores. El primero, *StemmerTokenizer*, para preprocesar el texto que luego sera vectorizados con bag-of-words. Y el segundo, *SplitTokenizer*, para preprocesar el texto catergorico separado por guiones para luego vectorizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9024bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "class StemmerTokenizer:\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        doc_tok = word_tokenize(doc.lower())\n",
    "        doc_tok = [t for t in doc_tok if t not in stop_words]\n",
    "        return [self.ps.stem(t) for t in doc_tok]\n",
    "    \n",
    "class SplitTokenizer:\n",
    "    def __init__(self, char=\"-\", col=\"\"):\n",
    "        self.char = char\n",
    "        self.col = col\n",
    "    def __call__(self, doc):\n",
    "        if self.col == \"production_companies\":\n",
    "            return doc.replace(\" \", \"_\").replace(\"Metro-Goldwyn-Mayer\", \"MGM\").split(self.char)\n",
    "        elif self.col == \"credits\":\n",
    "            tokens = doc.split(\"-\")\n",
    "            real_tokens = []\n",
    "            for i, tk in enumerate(tokens[:-1]):\n",
    "                if len(tokens[i+1].split()) == 1: tk = f\"{tk} {tokens[i+1]}\"\n",
    "                if len(tk.split())>1: real_tokens.append(tk)\n",
    "            return real_tokens\n",
    "        else:\n",
    "            return doc.replace(\" \", \"_\").split(self.char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este trabajo estudiamos dos *ColumnTransformer*. Uno de ellos es una version sin bag-of-words del otro, denotando por *ct_wo_bow* al primero, y *ct* al segundo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ColumnTransformer *ct*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MinMaxScaler: atributos_minmax\n",
    "* StandardScaler: atributos_st\n",
    "* BOW: overview, tagline\n",
    "* OneHot: atributos_onehot\n",
    "* OneHot(split): genres, credits, production_companies, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([\n",
    "    (\"OneHot\", \n",
    "     OneHotEncoder(handle_unknown=\"ignore\"), \n",
    "     atributos_onehot\n",
    "    ),\n",
    "    (\"MinMax\", \n",
    "     MinMaxScaler(), \n",
    "     atributos_minmax\n",
    "    ),\n",
    "    (\"Standard\", \n",
    "     StandardScaler(), \n",
    "     atributos_st\n",
    "    ),\n",
    "    (\"BOW1\", \n",
    "     CountVectorizer(tokenizer= StemmerTokenizer()), \n",
    "     \"overview\"\n",
    "    ),\n",
    "    (\"OneHot_split_genres\", \n",
    "     CountVectorizer(tokenizer= SplitTokenizer()), \n",
    "     \"genres\"\n",
    "    ),\n",
    "    (\"BOW2\", \n",
    "     CountVectorizer(tokenizer= StemmerTokenizer()), \n",
    "     \"tagline\"\n",
    "    ),\n",
    "    (\"OneHot_split_credits\", \n",
    "     CountVectorizer(tokenizer= SplitTokenizer(col=\"credits\")), \n",
    "     \"credits\"\n",
    "    ),\n",
    "    (\"OneHot_split_production_companies\", \n",
    "     CountVectorizer(tokenizer= SplitTokenizer(col=\"production_companies\")), \n",
    "     \"production_companies\"\n",
    "    ),\n",
    "    (\"OneHot_split_keywords\", \n",
    "     CountVectorizer(tokenizer= SplitTokenizer()), \n",
    "     \"keywords\"\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ColumnTransformer *ct_wo_bow*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_wo_bow = ColumnTransformer([\n",
    "    (\"OneHot\", \n",
    "     OneHotEncoder(handle_unknown=\"ignore\"), \n",
    "     atributos_onehot\n",
    "    ),\n",
    "    (\"MinMax\", \n",
    "     MinMaxScaler(), \n",
    "     atributos_minmax\n",
    "    ),\n",
    "    (\"Standard\", \n",
    "     StandardScaler(), \n",
    "     atributos_st\n",
    "    ),\n",
    "    (\"OneHot_split_genres\", \n",
    "     CountVectorizer(tokenizer= SplitTokenizer()), \n",
    "     \"genres\"\n",
    "    ),\n",
    "    (\"OneHot_split_credits\", \n",
    "     CountVectorizer(tokenizer= SplitTokenizer(col=\"credits\")), \n",
    "     \"credits\"\n",
    "    ),\n",
    "    (\"OneHot_split_production_companies\", \n",
    "     CountVectorizer(tokenizer= SplitTokenizer(col=\"production_companies\")), \n",
    "     \"production_companies\"\n",
    "    ),\n",
    "    (\"OneHot_split_keywords\", \n",
    "     CountVectorizer(tokenizer= SplitTokenizer()), \n",
    "     \"keywords\"\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4fa7c",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "<!-- Adicionalmente puede generar una nueva transformación que genere nuevas features y que se aplique antes del ColumnTransformer dentro del pipeline de los modelos. Investigar [`FunctionTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) para ver como implementar una transformación a partir de una función que tome un dataframe y entregue uno distinto en la salida.\n",
    "\n",
    "- Encodear ciclicamente los meses/días de las fechas de lanzamiento.\n",
    "- Contar cuantas veces aparecen en las peliculas ciertos personajes célebres.\n",
    "- Indicar si la pelicula es de una productora famosa o no.\n",
    "- Agrupar distintas keywords en categorías más generales.\n",
    "- Generar ratios con las variables numericas del dataset (como duración de la película/presupuesto).\n",
    "- Contar los diferentes generos similares que posee una pelicula.\n",
    "- Extraer vectores desde los overviews de las peliculas.\n",
    "- Contar el número de actores/productoras/géneros.\n",
    "- Etc... Usen su creatividad!\n",
    "\n",
    "Nuevamente, recuerde no utilizar ni los targets ni las labels para generar nuevas features.\n",
    "\n",
    "Nota: Este último paso no es requisito pero puede catapultarlos a la cima del tablero de las competencias. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideramos once variables, diseñadas manualmente a partir de los datos. Estas son:\n",
    "* *release_date_timestamp.* Variable continua del tiempo de estreno\n",
    "* *release_date_month.* Variable entera que identifica el mes de estreno\n",
    "* *release_date_day.* Variable entera que identifica el dia de estreno\n",
    "* *num_top_production_companies.* Numero de productoras que son populares\n",
    "* *num_top_artists.* Numero de artistas que son populares\n",
    "* *ratio(runtime, budget).* Proporcion entre la duracion de la pelicula y el presupuesto\n",
    "* *num_artists.* Numero de artistas\n",
    "* *num_genres.* Numero de generos\n",
    "* *num_production_companies.* Numero de productoras \n",
    "* *prop(num_top_production_companies).* Proporcion de productoras que son populares\n",
    "* *prop(num_top_artists).* Proporcion de artistas que son populares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c6cd3f6",
   "metadata": {
    "cell_id": "00016-5586ef61-95ea-4f5a-8ad0-796be8c78ac0",
    "deepnote_cell_height": 80.13333129882812,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166471,
    "scrolled": true,
    "source_hash": "2dc8e0f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionTransformer(func=<function feature_extractor at 0x000001D2D39AEC10>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_extractor(X):\n",
    "    movies = X.copy()\n",
    "    movies[\"release_date_timestamp\"] = movies['release_date'].apply(lambda x: x.timestamp())\n",
    "    movies[\"release_date_month\"] = movies['release_date'].dt.month\n",
    "    movies[\"release_date_day\"] = movies['release_date'].dt.day\n",
    "    \n",
    "    top_companies = ['Warner Bros. Pictures', 'Universal Pictures', 'Columbia Pictures',\n",
    "       'Paramount', '20th Century Fox', 'Canal+', 'New Line Cinema',\n",
    "       'Metro-Goldwyn-Mayer', 'Lionsgate', 'Relativity Media',\n",
    "       'StudioCanal', 'Touchstone Pictures', 'Walt Disney Pictures',\n",
    "       'DreamWorks Pictures', 'Miramax']\n",
    "    \n",
    "    movies[\"num_top_production_companies\"] = movies[\"production_companies\"].apply(\n",
    "        lambda x: \n",
    "        sum(\n",
    "            [int(comp in str(x)) for comp in top_companies]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    top_artists = ['Samuel L. Jackson', 'Frank Welker', 'Nicolas Cage',\n",
    "       'Bruce Willis', 'Robert De Niro', 'Matt Damon', 'Liam Neeson',\n",
    "       'Willem Dafoe', 'Morgan Freeman', 'J.K. Simmons', 'Steve Buscemi',\n",
    "       'Johnny Depp', 'John Goodman', 'Paul Giamatti', 'Stanley Tucci',\n",
    "       'Woody Harrelson', 'Brad Pitt', 'Mickie McGowan', 'John Leguizamo',\n",
    "       'Robin Williams', 'Sylvester Stallone', 'Tom Hanks',\n",
    "       'Michael Papajohn', 'Nicole Kidman', 'Thomas Rosales Jr.',\n",
    "       'James Franco', 'Harrison Ford', 'Ben Affleck', 'Owen Wilson',\n",
    "       'Stephen Root', 'Julianne Moore', 'Ben Kingsley',\n",
    "       'Antonio Banderas', 'Anthony Hopkins', 'Alec Baldwin',\n",
    "       'Joe Chrest', 'Bill Hader', 'Richard Jenkins', 'John C. Reilly',\n",
    "       'Bill Murray', 'John Hurt', 'Elizabeth Banks', 'Michael Caine',\n",
    "       'Ewan McGregor', 'Keith David', 'Susan Sarandon',\n",
    "       'Fred Tatasciore', 'Bob Bergen', 'Scarlett Johansson',\n",
    "       'Keanu Reeves']\n",
    "    \n",
    "    movies[\"num_top_artists\"] = movies[\"credits\"].apply(\n",
    "        lambda x: \n",
    "        sum(\n",
    "            [int(art in str(x).replace(\"-\", \" \")) for art in top_artists]\n",
    "        )\n",
    "    )\n",
    "    movies[\"ratio(runtime, budget)\"] = (movies[\"runtime\"]/(1+movies[\"budget\"]))\n",
    "    movies[\"num_artists\"] = movies[\"credits\"].apply(\n",
    "        lambda x:\n",
    "        sum(\n",
    "            [len(str(x).split(\"-\"))]\n",
    "        )\n",
    "    )\n",
    "    movies[\"num_genres\"] = movies[\"genres\"].apply(\n",
    "        lambda x: len(str(x).split(\"-\"))\n",
    "    )\n",
    "    movies[\"num_production_companies\"] = movies[\"production_companies\"].apply(\n",
    "        lambda x: len(str(x).split(\"-\"))\n",
    "    )\n",
    "    movies[\"prop(num_top_production_companies)\"] = movies[\"num_top_production_companies\"]/(1+movies[\"num_production_companies\"])\n",
    "    movies[\"prop(num_top_artists)\"] = movies[\"num_top_artists\"]/(1+movies[\"num_artists\"])\n",
    "    \n",
    "    return movies\n",
    "\n",
    "feature_tranformer = FunctionTransformer(feature_extractor)\n",
    "feature_tranformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89427e68",
   "metadata": {
    "cell_id": "ec9c0f8a2b044f858858ef3c3248c239",
    "deepnote_cell_height": 102,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "source": [
    "En esta subseccion de realizo una separacion de los datos, para entrenamiento y evaluacion. Adicionalmente, se contruyeron dos column transformers a partir de los datos, con el objetivo de preprocesar los datos vectorialmente. Finalmente, se añadieron once variables diseñadas manualmente con un FunctionTransformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869eda7",
   "metadata": {
    "cell_id": "00018-d6de5b4a-3ce2-4aaa-9421-121c4fcaf3b5",
    "deepnote_cell_height": 117.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Clasificación\n",
    "\n",
    "### 4.1 Dummy y Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta seccion consiste en contruir dos tipos de clasificadores simples para poder comparar en un futuro otros clasificadores candidatos para poder resolver nuestra tarea de clasifiacion.\n",
    "\n",
    "El primero de ellos es un modelo Dummy (azaroso) y el otro un Baseline (mejor que el primero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65949e",
   "metadata": {},
   "source": [
    "<!-- *En esta sección crearán el modelo más básico posible que resuelva el problema. La idea de este modelo usarlo como comparación para que en el siguiente paso lo puedan mejorar.*\n",
    "\n",
    "- Generar un modelo Dummy con estrategia estratificada que les permita comparar más adelante si su baseline de clasificación es mejor que el azar.\n",
    "- Generar un pipeline para la clasificación con un clasificador relativamente sencillo a la salida (a su elección, recomendado: arbol de decisión).\n",
    "- Evaluar ambos modelos según las métricas de evaluación y reportar. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de clasificación azaroza proporcional al número de datos por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ef6e328",
   "metadata": {
    "cell_id": "00020-830185e8-7ee6-44a5-89f0-ccb5e5d4ef76",
    "deepnote_cell_height": 80.13333129882812,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166499,
    "source_hash": "3e943dc6"
   },
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"stratified\", random_state=0)\n",
    "dummy_clf.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "y_pred_dummy = dummy_clf.predict(X_clf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo baseline: Random forest (max_depth = 8, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67670606",
   "metadata": {
    "cell_id": "767a72b6c3ca4caa86bf94ce7f00421b",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe_clf_baseline = Pipeline(\n",
    "    steps=[\n",
    "        (\"FunctionTransformer\", feature_tranformer),\n",
    "        (\"ColumnTransformer\", ct),\n",
    "        (\"clf\", RandomForestClassifier(class_weight=\"balanced\", max_depth=8, random_state=0))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d478512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('FunctionTransformer',\n",
       "                 FunctionTransformer(func=<function feature_extractor at 0x000001D2D39AEC10>)),\n",
       "                ('ColumnTransformer',\n",
       "                 ColumnTransformer(transformers=[('OneHot',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['original_language',\n",
       "                                                   'clusters_keywords',\n",
       "                                                   'clusters_overview',\n",
       "                                                   'clusters_tagline']),\n",
       "                                                 ('MinMax', MinMaxScaler(),\n",
       "                                                  ['budget',\n",
       "                                                   'release_date_mon...\n",
       "                                                  'credits'),\n",
       "                                                 ('OneHot_split_production_companies',\n",
       "                                                  CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2D915AA30>),\n",
       "                                                  'production_companies'),\n",
       "                                                 ('OneHot_split_keywords',\n",
       "                                                  CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2D915A8E0>),\n",
       "                                                  'keywords')])),\n",
       "                ('clf',\n",
       "                 RandomForestClassifier(class_weight='balanced', max_depth=8,\n",
       "                                        random_state=0))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipe_clf_baseline.fit(X_clf_train, y_clf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "592f3475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "y_pred_baseline = pipe_clf_baseline.predict(X_clf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificador Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f76618d",
   "metadata": {
    "cell_id": "57e26f4f95c440a2be8794ea16cb12db",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador Dummy\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.20      0.23      0.21       252\n",
      "Mostly Positive       0.47      0.43      0.45       621\n",
      "       Negative       0.05      0.05      0.05        37\n",
      "       Positive       0.25      0.25      0.25       343\n",
      "  Very Positive       0.04      0.05      0.05        38\n",
      "\n",
      "       accuracy                           0.32      1291\n",
      "      macro avg       0.20      0.20      0.20      1291\n",
      "   weighted avg       0.33      0.32      0.33      1291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Clasificador Dummy\\n\"+classification_report(y_clf_test, y_pred_dummy, zero_division=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificador baseline: RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d35939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador baseline: RF\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.31      0.69      0.43       252\n",
      "Mostly Positive       0.56      0.18      0.27       621\n",
      "       Negative       0.17      0.03      0.05        37\n",
      "       Positive       0.44      0.66      0.53       343\n",
      "  Very Positive       0.29      0.11      0.15        38\n",
      "\n",
      "       accuracy                           0.40      1291\n",
      "      macro avg       0.35      0.33      0.28      1291\n",
      "   weighted avg       0.46      0.40      0.36      1291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Clasificador baseline: RF\\n\"+classification_report(y_clf_test, y_pred_baseline, zero_division=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporte de resultados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1d3a8",
   "metadata": {
    "cell_id": "8b46d8fd6f9544b199b67020abad2562",
    "deepnote_cell_height": 69.93333435058594,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "```\n",
    "Dummy: 0.20 f1_macro\n",
    "Baseline: 0.28 f1_macro\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo Dummy obtiene un F1-macro del 20%. Mientras que el modelo Baseline un F1-macro del 28%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8e074",
   "metadata": {
    "cell_id": "00021-c294ef41-853d-4297-b051-d5d4e6577715",
    "deepnote_cell_height": 61.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "### 4.2 Búsqueda del mejor modelo de Clasificación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta seccion nuestro objetivo es encontrar un modelo mejor que el Baseline. Para esto se realizara una busqueda del mejor modelo de clasifiacion con la tecnica de GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41b3d0",
   "metadata": {},
   "source": [
    "<!-- *Aquí deberán mejorar del modelo de clasificación al variar los algoritmos/hiperparámetros que están ocupando.*\n",
    "\n",
    "- Generar una nueva `Pipeline` enfocada en buscar el mejor modelo usando GridSearch.\n",
    "- Usar **`GridSearchCV`** o **`HalvingGridSearchCV`** para tunear hiperparámetros. La primera demorará más que la segunda pero les traerá potencialmente mejores resultados. Pueden probar también [`OptunaSearchCV`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.OptunaSearchCV.html) de la librería [`Optuna`](https://optuna.org/) , la cuál es bastante popular para buscar modelos de redes neuronales.\n",
    "- Agregar técnicas de seleccion de atributos, como también usar mejores clasificadores y explorar sus hiperparámetros. \n",
    "- Probar distintos parámetros para las transformaciones de datos, seleccion de atributos, clasificadores, etc...\n",
    "- Probar modelos basados en gradient boosting/bagging. **Recomendación fuerte:** Probar [`LightGMB`](https://lightgbm.readthedocs.io/en/latest/) o [`xgboost`](https://xgboost.readthedocs.io/en/stable/).\n",
    "- Probar activando/descativando los procesadores de texto, de categorías, etc...\n",
    "- Recuerden setear la búsqueda para optimizar la métrica que se evalua en la competencia.\n",
    "\n",
    "Algunas notas interesantes sobre este proceso: \n",
    "\n",
    "- No se les pide rendimientos cercanos al 100% de la métrica para concretar exitosamente el proyecto. Por otra parte, celebren cada progreso que obtengan.\n",
    "- Hagan grillas computables: Si la grilla se va a demorar 1/3 la edad del universo en explorarse completamente, entonces achíquenla a algo que sepan que va a terminar. \n",
    "- Aprovechen el procesamiento paralelo (con `njobs`) para acelerar la búsqueda. Sin embargo, si tienen problemas con la memoria RAM, reduzca la cantidad de jobs a algo que su computador/interprete web pueda procesar.\n",
    "\n",
    "**Al final de este proceso, seleccione el mejor modelo de clasificación encontrado, prediga las labels del test set de la competencia y envíelos a Codalab.** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de elegir que modelos utilizar, estudiaremos los siguientes seis modelos de clasificación:\n",
    "* MLPClassifier\n",
    "* LinearSVC\n",
    "* KNeighborsClassifier\n",
    "* SVC\n",
    "* RandomForestClassifier\n",
    "* DecisionTreeClassifier\n",
    "\n",
    "De estos modelos, solo se usaran en la busqueda aquellos que (1) por simple inspección obtengan un desempeño razonable y (2) el tiempo de entrenamiento sea apropiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c584952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipe_model(model_class, params, params_selector):\n",
    "    model = model_class(**params)\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"FunctionTransformer\", feature_tranformer),\n",
    "            (\"ColumnTransformer\", ct),\n",
    "            (\"selector\", SelectPercentile(f_classif, **params_selector)),\n",
    "            ('clf', model)\n",
    "        ]\n",
    "     )\n",
    "    pipe.fit(X_clf_train, y_clf_train)\n",
    "    y_pred = pipe.predict(X_clf_test)\n",
    "    print(\n",
    "        f\"Clasificador: {model_class.__name__}\\n\"+classification_report(y_clf_test, y_pred, zero_division=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef887207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador: MLPClassifier\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.46      0.21      0.28       252\n",
      "Mostly Positive       0.53      0.78      0.63       621\n",
      "       Negative       1.00      0.00      0.00        37\n",
      "       Positive       0.56      0.44      0.49       343\n",
      "  Very Positive       1.00      0.00      0.00        38\n",
      "\n",
      "       accuracy                           0.53      1291\n",
      "      macro avg       0.71      0.28      0.28      1291\n",
      "   weighted avg       0.55      0.53      0.49      1291\n",
      "\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pipe_model(\n",
    "    MLPClassifier, \n",
    "    params={\n",
    "        \"hidden_layer_sizes\": (50,), \n",
    "        \"activation\": \"relu\", \n",
    "        \"solver\": \"adam\", \n",
    "        \"batch_size\": 144,\n",
    "        \"learning_rate\": \"constant\",\n",
    "        \"learning_rate_init\": 0.001,\n",
    "        \"early_stopping\": True,\n",
    "        \"max_iter\": 50\n",
    "    }, \n",
    "    params_selector={\"percentile\": 95}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "F1-macro: 0.28\n",
    "Tiempo de entrenamiento: 2min 57s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9fd9b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador: LinearSVC\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.42      0.43      0.43       252\n",
      "Mostly Positive       0.60      0.57      0.58       621\n",
      "       Negative       0.12      0.05      0.07        37\n",
      "       Positive       0.53      0.64      0.58       343\n",
      "  Very Positive       0.12      0.03      0.04        38\n",
      "\n",
      "       accuracy                           0.53      1291\n",
      "      macro avg       0.36      0.35      0.34      1291\n",
      "   weighted avg       0.52      0.53      0.52      1291\n",
      "\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pipe_model(\n",
    "    LinearSVC, \n",
    "    params={\"class_weight\": \"balanced\", \"C\": 0.001}, \n",
    "    params_selector={\"percentile\": 75}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "F1-macro: 0.34\n",
    "Tiempo de entrenamiento: 14.5 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebead9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador: KNeighborsClassifier\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.22      0.79      0.35       252\n",
      "Mostly Positive       0.43      0.15      0.22       621\n",
      "       Negative       0.03      0.08      0.04        37\n",
      "       Positive       0.64      0.16      0.25       343\n",
      "  Very Positive       1.00      0.00      0.00        38\n",
      "\n",
      "       accuracy                           0.27      1291\n",
      "      macro avg       0.46      0.24      0.17      1291\n",
      "   weighted avg       0.45      0.27      0.24      1291\n",
      "\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pipe_model(\n",
    "    KNeighborsClassifier, \n",
    "    params={\"n_neighbors\": 3}, \n",
    "    params_selector={\"percentile\": 100}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "F1-macro: 0.17\n",
    "Tiempo de entrenamiento: 14.9 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c9467e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador: SVC\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       1.00      0.00      0.00       252\n",
      "Mostly Positive       1.00      0.00      0.00       621\n",
      "       Negative       0.03      1.00      0.06        37\n",
      "       Positive       1.00      0.00      0.00       343\n",
      "  Very Positive       1.00      0.00      0.00        38\n",
      "\n",
      "       accuracy                           0.03      1291\n",
      "      macro avg       0.81      0.20      0.01      1291\n",
      "   weighted avg       0.97      0.03      0.00      1291\n",
      "\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pipe_model(\n",
    "    SVC, \n",
    "    params={\"class_weight\": \"balanced\", \"gamma\": \"auto\", \"kernel\": \"rbf\"}, \n",
    "    params_selector={\"percentile\": 100}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "F1-macro: 0.01\n",
    "Tiempo de entrenamiento: 1min 23s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19b1bc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador: RandomForestClassifier\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.32      0.67      0.44       252\n",
      "Mostly Positive       0.53      0.17      0.26       621\n",
      "       Negative       1.00      0.00      0.00        37\n",
      "       Positive       0.44      0.73      0.55       343\n",
      "  Very Positive       0.43      0.08      0.13        38\n",
      "\n",
      "       accuracy                           0.41      1291\n",
      "      macro avg       0.54      0.33      0.28      1291\n",
      "   weighted avg       0.48      0.41      0.36      1291\n",
      "\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pipe_model(\n",
    "    RandomForestClassifier, \n",
    "    params={\"class_weight\": \"balanced\", \"max_depth\": 15, \"n_estimators\": 150, \"random_state\": 0}, \n",
    "    params_selector={\"percentile\": 100}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "F1-macro: 0.28\n",
    "Tiempo de entrenamiento: 16.4 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03f8ba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador: DecisionTreeClassifier\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.36      0.25      0.30       252\n",
      "Mostly Positive       0.52      0.67      0.59       621\n",
      "       Negative       0.33      0.03      0.05        37\n",
      "       Positive       0.48      0.41      0.44       343\n",
      "  Very Positive       0.23      0.13      0.17        38\n",
      "\n",
      "       accuracy                           0.48      1291\n",
      "      macro avg       0.39      0.30      0.31      1291\n",
      "   weighted avg       0.47      0.48      0.46      1291\n",
      "\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_pipe_model(\n",
    "    DecisionTreeClassifier, \n",
    "    params={\"class_weight\": None, \"max_depth\": 15, \"random_state\": 0}, \n",
    "    params_selector={\"percentile\": 100}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "F1-macro: 0.31\n",
    "Tiempo de entrenamiento: 17.5 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los resultados preliminares, solo consideramos tres de los seis clasificadores para encontrar cual de ellos es mejor. Estos son: DecisionTreeClassifier, RandomForestClassifier, LinearSVC; con las siguientes grillas de parametros:\n",
    "\n",
    "1. **DecisionTreeClassifier**:\n",
    "    * selector percentile: [25, 50, 75, 95, 100]\n",
    "    * criterion: \"gini\"\n",
    "    * random_state: 42\n",
    "    * class_weight: [\"balanced\", None]\n",
    "    * max_depth: [5, 10, 15]\n",
    "    * ColumnTransformer: [ct, ct_wo_bow]\n",
    "2. **RandomForestClassifier**:\n",
    "    * selector percentile: [25, 50, 75, 95, 100]\n",
    "    * criterion: \"gini\"\n",
    "    * class_weight: \"balanced\"\n",
    "    * random_state: 42\n",
    "    * max_depth\": [5, 10, 15]\n",
    "    * n_estimators: [50, 70, 100]\n",
    "    * ColumnTransformer: [ct, ct_wo_bow]\n",
    "3. **LinearSVC**:\n",
    "    * selector percentile: [25, 50, 75, 95, 100]\n",
    "    * random_state: 42\n",
    "    * class_weight: [\"balanced\", None]\n",
    "    * C: [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "    * ColumnTransformer: [ct, ct_wo_bow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definición de la grilla**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc662277",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"FunctionTransformer\", feature_tranformer),\n",
    "            (\"ColumnTransformer\", ct),\n",
    "            (\"selector\", SelectPercentile(f_classif, percentile=50)),\n",
    "            ('clf', LinearSVC())\n",
    "        ]\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94052ed8",
   "metadata": {
    "cell_id": "00022-b323dc3e-6fa0-4d50-8a50-56ad708178ba",
    "deepnote_cell_height": 80.13333129882812,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1637954166499,
    "source_hash": "8f673430",
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    # grilla 1: DecisionTreeClassifier\n",
    "    {\n",
    "        \"selector__percentile\": [25, 50, 75, 95, 100],\n",
    "        \"clf\": [DecisionTreeClassifier(criterion=\"gini\", random_state=42)],\n",
    "        \"clf__class_weight\": [\"balanced\", None],\n",
    "        \"clf__max_depth\": [5, 10, 15],\n",
    "        \"ColumnTransformer\": [ct, ct_wo_bow]\n",
    "    },\n",
    "    # grilla 2: RandomForestClassifier\n",
    "    {\n",
    "        \"selector__percentile\": [25, 50, 75, 95, 100],\n",
    "        \"clf\": [RandomForestClassifier(criterion=\"gini\", class_weight=\"balanced\", random_state=42)],\n",
    "        \"clf__max_depth\": [5, 10, 15],\n",
    "        \"clf__n_estimators\": [50, 70, 100],\n",
    "        \"ColumnTransformer\": [ct, ct_wo_bow]\n",
    "    },\n",
    "    # grilla 3: LinearSVC\n",
    "    {\n",
    "        \"selector__percentile\": [25, 50, 75, 95, 100],\n",
    "        \"clf\": [LinearSVC(random_state=42)],\n",
    "        \"clf__class_weight\": [\"balanced\", None],\n",
    "        \"clf__C\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "        \"ColumnTransformer\": [ct, ct_wo_bow]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31fd5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, param_grid, n_jobs=-1, scoring='f1_macro', verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bc96b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 250 candidates, totalling 1250 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('FunctionTransformer',\n",
       "                                        FunctionTransformer(func=<function feature_extractor at 0x000001D2D39AEC10>)),\n",
       "                                       ('ColumnTransformer',\n",
       "                                        ColumnTransformer(transformers=[('OneHot',\n",
       "                                                                         OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                         ['original_language',\n",
       "                                                                          'clusters_keywords',\n",
       "                                                                          'clusters_overview',\n",
       "                                                                          'clusters_tagline']),\n",
       "                                                                        ('MinMax',\n",
       "                                                                         MinMaxScaler(),\n",
       "                                                                         ['bu...\n",
       "                                                                                 'production_companies'),\n",
       "                                                                                ('OneHot_split_keywords',\n",
       "                                                                                 CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2D915AD30>),\n",
       "                                                                                 'keywords')])],\n",
       "                          'clf': [LinearSVC(C=0.001, class_weight='balanced',\n",
       "                                            random_state=42)],\n",
       "                          'clf__C': [0.0001, 0.001, 0.01, 0.1, 1],\n",
       "                          'clf__class_weight': ['balanced', None],\n",
       "                          'selector__percentile': [25, 50, 75, 95, 100]}],\n",
       "             scoring='f1_macro', verbose=10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_clf_train, y_clf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "691fe7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3695961508470219"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f6ce28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('FunctionTransformer',\n",
       "   FunctionTransformer(func=<function feature_extractor at 0x000001D2D39AEC10>)),\n",
       "  ('ColumnTransformer',\n",
       "   ColumnTransformer(transformers=[('OneHot',\n",
       "                                    OneHotEncoder(handle_unknown='ignore'),\n",
       "                                    ['original_language', 'clusters_keywords',\n",
       "                                     'clusters_overview', 'clusters_tagline']),\n",
       "                                   ('MinMax', MinMaxScaler(),\n",
       "                                    ['budget', 'release_date_month',\n",
       "                                     'release_date_day',\n",
       "                                     'num_top_production_companies',\n",
       "                                     'num_top_artists', 'ratio(runtime, budget)',\n",
       "                                     'num_artists', 'num_genres',\n",
       "                                     'num_product...\n",
       "                                   ('OneHot_split_credits',\n",
       "                                    CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DC20BBE0>),\n",
       "                                    'credits'),\n",
       "                                   ('OneHot_split_production_companies',\n",
       "                                    CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DC20BA30>),\n",
       "                                    'production_companies'),\n",
       "                                   ('OneHot_split_keywords',\n",
       "                                    CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DA83D070>),\n",
       "                                    'keywords')])),\n",
       "  ('selector', SelectPercentile(percentile=95)),\n",
       "  ('clf', LinearSVC(C=0.001, class_weight='balanced', random_state=42))],\n",
       " 'verbose': False,\n",
       " 'FunctionTransformer': FunctionTransformer(func=<function feature_extractor at 0x000001D2D39AEC10>),\n",
       " 'ColumnTransformer': ColumnTransformer(transformers=[('OneHot',\n",
       "                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                  ['original_language', 'clusters_keywords',\n",
       "                                   'clusters_overview', 'clusters_tagline']),\n",
       "                                 ('MinMax', MinMaxScaler(),\n",
       "                                  ['budget', 'release_date_month',\n",
       "                                   'release_date_day',\n",
       "                                   'num_top_production_companies',\n",
       "                                   'num_top_artists', 'ratio(runtime, budget)',\n",
       "                                   'num_artists', 'num_genres',\n",
       "                                   'num_product...\n",
       "                                 ('OneHot_split_credits',\n",
       "                                  CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DC20BBE0>),\n",
       "                                  'credits'),\n",
       "                                 ('OneHot_split_production_companies',\n",
       "                                  CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DC20BA30>),\n",
       "                                  'production_companies'),\n",
       "                                 ('OneHot_split_keywords',\n",
       "                                  CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DA83D070>),\n",
       "                                  'keywords')]),\n",
       " 'selector': SelectPercentile(percentile=95),\n",
       " 'clf': LinearSVC(C=0.001, class_weight='balanced', random_state=42),\n",
       " 'FunctionTransformer__accept_sparse': False,\n",
       " 'FunctionTransformer__check_inverse': True,\n",
       " 'FunctionTransformer__func': <function __main__.feature_extractor(X)>,\n",
       " 'FunctionTransformer__inv_kw_args': None,\n",
       " 'FunctionTransformer__inverse_func': None,\n",
       " 'FunctionTransformer__kw_args': None,\n",
       " 'FunctionTransformer__validate': False,\n",
       " 'ColumnTransformer__n_jobs': None,\n",
       " 'ColumnTransformer__remainder': 'drop',\n",
       " 'ColumnTransformer__sparse_threshold': 0.3,\n",
       " 'ColumnTransformer__transformer_weights': None,\n",
       " 'ColumnTransformer__transformers': [('OneHot',\n",
       "   OneHotEncoder(handle_unknown='ignore'),\n",
       "   ['original_language',\n",
       "    'clusters_keywords',\n",
       "    'clusters_overview',\n",
       "    'clusters_tagline']),\n",
       "  ('MinMax',\n",
       "   MinMaxScaler(),\n",
       "   ['budget',\n",
       "    'release_date_month',\n",
       "    'release_date_day',\n",
       "    'num_top_production_companies',\n",
       "    'num_top_artists',\n",
       "    'ratio(runtime, budget)',\n",
       "    'num_artists',\n",
       "    'num_genres',\n",
       "    'num_production_companies',\n",
       "    'prop(num_top_production_companies)',\n",
       "    'prop(num_top_artists)']),\n",
       "  ('Standard', StandardScaler(), ['runtime', 'release_date_timestamp']),\n",
       "  ('OneHot_split_genres',\n",
       "   CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DC20BCD0>),\n",
       "   'genres'),\n",
       "  ('OneHot_split_credits',\n",
       "   CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DC20BBE0>),\n",
       "   'credits'),\n",
       "  ('OneHot_split_production_companies',\n",
       "   CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DC20BA30>),\n",
       "   'production_companies'),\n",
       "  ('OneHot_split_keywords',\n",
       "   CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DA83D070>),\n",
       "   'keywords')],\n",
       " 'ColumnTransformer__verbose': False,\n",
       " 'ColumnTransformer__verbose_feature_names_out': True,\n",
       " 'ColumnTransformer__OneHot': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'ColumnTransformer__MinMax': MinMaxScaler(),\n",
       " 'ColumnTransformer__Standard': StandardScaler(),\n",
       " 'ColumnTransformer__OneHot_split_genres': CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DC20BCD0>),\n",
       " 'ColumnTransformer__OneHot_split_credits': CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DC20BBE0>),\n",
       " 'ColumnTransformer__OneHot_split_production_companies': CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DC20BA30>),\n",
       " 'ColumnTransformer__OneHot_split_keywords': CountVectorizer(tokenizer=<__main__.SplitTokenizer object at 0x000001D2DA83D070>),\n",
       " 'ColumnTransformer__OneHot__categories': 'auto',\n",
       " 'ColumnTransformer__OneHot__drop': None,\n",
       " 'ColumnTransformer__OneHot__dtype': numpy.float64,\n",
       " 'ColumnTransformer__OneHot__handle_unknown': 'ignore',\n",
       " 'ColumnTransformer__OneHot__sparse': True,\n",
       " 'ColumnTransformer__MinMax__clip': False,\n",
       " 'ColumnTransformer__MinMax__copy': True,\n",
       " 'ColumnTransformer__MinMax__feature_range': (0, 1),\n",
       " 'ColumnTransformer__Standard__copy': True,\n",
       " 'ColumnTransformer__Standard__with_mean': True,\n",
       " 'ColumnTransformer__Standard__with_std': True,\n",
       " 'ColumnTransformer__OneHot_split_genres__analyzer': 'word',\n",
       " 'ColumnTransformer__OneHot_split_genres__binary': False,\n",
       " 'ColumnTransformer__OneHot_split_genres__decode_error': 'strict',\n",
       " 'ColumnTransformer__OneHot_split_genres__dtype': numpy.int64,\n",
       " 'ColumnTransformer__OneHot_split_genres__encoding': 'utf-8',\n",
       " 'ColumnTransformer__OneHot_split_genres__input': 'content',\n",
       " 'ColumnTransformer__OneHot_split_genres__lowercase': True,\n",
       " 'ColumnTransformer__OneHot_split_genres__max_df': 1.0,\n",
       " 'ColumnTransformer__OneHot_split_genres__max_features': None,\n",
       " 'ColumnTransformer__OneHot_split_genres__min_df': 1,\n",
       " 'ColumnTransformer__OneHot_split_genres__ngram_range': (1, 1),\n",
       " 'ColumnTransformer__OneHot_split_genres__preprocessor': None,\n",
       " 'ColumnTransformer__OneHot_split_genres__stop_words': None,\n",
       " 'ColumnTransformer__OneHot_split_genres__strip_accents': None,\n",
       " 'ColumnTransformer__OneHot_split_genres__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'ColumnTransformer__OneHot_split_genres__tokenizer': <__main__.SplitTokenizer at 0x1d2dc20bcd0>,\n",
       " 'ColumnTransformer__OneHot_split_genres__vocabulary': None,\n",
       " 'ColumnTransformer__OneHot_split_credits__analyzer': 'word',\n",
       " 'ColumnTransformer__OneHot_split_credits__binary': False,\n",
       " 'ColumnTransformer__OneHot_split_credits__decode_error': 'strict',\n",
       " 'ColumnTransformer__OneHot_split_credits__dtype': numpy.int64,\n",
       " 'ColumnTransformer__OneHot_split_credits__encoding': 'utf-8',\n",
       " 'ColumnTransformer__OneHot_split_credits__input': 'content',\n",
       " 'ColumnTransformer__OneHot_split_credits__lowercase': True,\n",
       " 'ColumnTransformer__OneHot_split_credits__max_df': 1.0,\n",
       " 'ColumnTransformer__OneHot_split_credits__max_features': None,\n",
       " 'ColumnTransformer__OneHot_split_credits__min_df': 1,\n",
       " 'ColumnTransformer__OneHot_split_credits__ngram_range': (1, 1),\n",
       " 'ColumnTransformer__OneHot_split_credits__preprocessor': None,\n",
       " 'ColumnTransformer__OneHot_split_credits__stop_words': None,\n",
       " 'ColumnTransformer__OneHot_split_credits__strip_accents': None,\n",
       " 'ColumnTransformer__OneHot_split_credits__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'ColumnTransformer__OneHot_split_credits__tokenizer': <__main__.SplitTokenizer at 0x1d2dc20bbe0>,\n",
       " 'ColumnTransformer__OneHot_split_credits__vocabulary': None,\n",
       " 'ColumnTransformer__OneHot_split_production_companies__analyzer': 'word',\n",
       " 'ColumnTransformer__OneHot_split_production_companies__binary': False,\n",
       " 'ColumnTransformer__OneHot_split_production_companies__decode_error': 'strict',\n",
       " 'ColumnTransformer__OneHot_split_production_companies__dtype': numpy.int64,\n",
       " 'ColumnTransformer__OneHot_split_production_companies__encoding': 'utf-8',\n",
       " 'ColumnTransformer__OneHot_split_production_companies__input': 'content',\n",
       " 'ColumnTransformer__OneHot_split_production_companies__lowercase': True,\n",
       " 'ColumnTransformer__OneHot_split_production_companies__max_df': 1.0,\n",
       " 'ColumnTransformer__OneHot_split_production_companies__max_features': None,\n",
       " 'ColumnTransformer__OneHot_split_production_companies__min_df': 1,\n",
       " 'ColumnTransformer__OneHot_split_production_companies__ngram_range': (1, 1),\n",
       " 'ColumnTransformer__OneHot_split_production_companies__preprocessor': None,\n",
       " 'ColumnTransformer__OneHot_split_production_companies__stop_words': None,\n",
       " 'ColumnTransformer__OneHot_split_production_companies__strip_accents': None,\n",
       " 'ColumnTransformer__OneHot_split_production_companies__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'ColumnTransformer__OneHot_split_production_companies__tokenizer': <__main__.SplitTokenizer at 0x1d2dc20ba30>,\n",
       " 'ColumnTransformer__OneHot_split_production_companies__vocabulary': None,\n",
       " 'ColumnTransformer__OneHot_split_keywords__analyzer': 'word',\n",
       " 'ColumnTransformer__OneHot_split_keywords__binary': False,\n",
       " 'ColumnTransformer__OneHot_split_keywords__decode_error': 'strict',\n",
       " 'ColumnTransformer__OneHot_split_keywords__dtype': numpy.int64,\n",
       " 'ColumnTransformer__OneHot_split_keywords__encoding': 'utf-8',\n",
       " 'ColumnTransformer__OneHot_split_keywords__input': 'content',\n",
       " 'ColumnTransformer__OneHot_split_keywords__lowercase': True,\n",
       " 'ColumnTransformer__OneHot_split_keywords__max_df': 1.0,\n",
       " 'ColumnTransformer__OneHot_split_keywords__max_features': None,\n",
       " 'ColumnTransformer__OneHot_split_keywords__min_df': 1,\n",
       " 'ColumnTransformer__OneHot_split_keywords__ngram_range': (1, 1),\n",
       " 'ColumnTransformer__OneHot_split_keywords__preprocessor': None,\n",
       " 'ColumnTransformer__OneHot_split_keywords__stop_words': None,\n",
       " 'ColumnTransformer__OneHot_split_keywords__strip_accents': None,\n",
       " 'ColumnTransformer__OneHot_split_keywords__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'ColumnTransformer__OneHot_split_keywords__tokenizer': <__main__.SplitTokenizer at 0x1d2da83d070>,\n",
       " 'ColumnTransformer__OneHot_split_keywords__vocabulary': None,\n",
       " 'selector__percentile': 95,\n",
       " 'selector__score_func': <function sklearn.feature_selection._univariate_selection.f_classif(X, y)>,\n",
       " 'clf__C': 0.001,\n",
       " 'clf__class_weight': 'balanced',\n",
       " 'clf__dual': True,\n",
       " 'clf__fit_intercept': True,\n",
       " 'clf__intercept_scaling': 1,\n",
       " 'clf__loss': 'squared_hinge',\n",
       " 'clf__max_iter': 1000,\n",
       " 'clf__multi_class': 'ovr',\n",
       " 'clf__penalty': 'l2',\n",
       " 'clf__random_state': 42,\n",
       " 'clf__tol': 0.0001,\n",
       " 'clf__verbose': 0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipe = gs.best_estimator_\n",
    "best_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor modelo. *LinearSVC*\n",
    "\n",
    "    selector percentile: 95\n",
    "    random_state: 42\n",
    "    class_weight: \"balanced\"\n",
    "    C: 0.001\n",
    "    ColumnTransformer: ct_wo_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score en testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0078f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clasificador: best_pipe\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.42      0.42      0.42       252\n",
      "Mostly Positive       0.59      0.56      0.57       621\n",
      "       Negative       0.23      0.14      0.17        37\n",
      "       Positive       0.53      0.65      0.58       343\n",
      "  Very Positive       0.11      0.03      0.04        38\n",
      "\n",
      "       accuracy                           0.53      1291\n",
      "      macro avg       0.37      0.36      0.36      1291\n",
      "   weighted avg       0.52      0.53      0.52      1291\n",
      "\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipe = best_pipe\n",
    "pipe.fit(X_clf_train, y_clf_train)\n",
    "y_pred = pipe.predict(X_clf_test)\n",
    "print(\n",
    "    f\"Clasificador: best_pipe\\n\"+classification_report(y_clf_test, y_pred, zero_division=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis de resultados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-macro\n",
    "```\n",
    "Dummy: 0.20\n",
    "Baseline: 0.28\n",
    "Candidato: 0.36\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del gridsearch, se encuentra un mejor modelo con un F1-macro del 36%. Mejor que el modelo Baseline. El modelo encontrado es un SVC con kernel lineal, un modelo simple pero que obtiene mejores resultados que el resto de clasificadores considerados en la grilla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicción de datos de la competencia**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar los datos de la competencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFiles(predict_data, clf_pipe):\n",
    "    \"\"\"Genera los archivos a subir en CodaLab\n",
    "\n",
    "    Input\n",
    "    predict_data: Dataframe con los datos de entrada a predecir\n",
    "    clf_pipe: pipeline del clf\n",
    "\n",
    "    Ouput\n",
    "    archivo de txt\n",
    "    \"\"\"\n",
    "    y_pred_clf = clf_pipe.predict(predict_data)\n",
    "    \n",
    "    with open('./predictions_clf.txt', 'w') as f:\n",
    "        for item in y_pred_clf:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    with ZipFile('predictions_clf.zip', 'w') as zipObj2:\n",
    "        zipObj2.write('predictions_clf.txt')\n",
    "\n",
    "    os.remove(\"predictions_clf.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48fa03c6",
   "metadata": {
    "cell_id": "12d71cdc88dd4c06b9a7e1696853f7b2",
    "deepnote_cell_height": 66,
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters_keywords</th>\n",
       "      <th>clusters_overview</th>\n",
       "      <th>clusters_tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>656 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    clusters_keywords clusters_overview clusters_tagline\n",
       "0                  11                 1                8\n",
       "1                   9                 7               11\n",
       "2                   5                 5                2\n",
       "3                  14                 5                3\n",
       "4                  13                11                3\n",
       "..                ...               ...              ...\n",
       "651                 9                11                0\n",
       "652                 1                 0                0\n",
       "653                 9                 1                5\n",
       "654                14                11                8\n",
       "655                 1                 7                8\n",
       "\n",
       "[656 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pickle.load(open(\"test.pickle\", \"rb\"))\n",
    "test[\"release_date\"] = test[\"release_date\"].apply(pd.to_datetime)\n",
    "test = test.fillna(\"\")\n",
    "test = test.reset_index(drop=True)\n",
    "test_clusters_BERT = pickle.load(open(\"test_clusters_BERT.pickle\", \"rb\"))\n",
    "test = test.merge(test_clusters_BERT, how=\"left\", on=\"title\")\n",
    "test[\"clusters_keywords clusters_overview clusters_tagline\".split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipe = pipe\n",
    "predict_data = test\n",
    "generateFiles(predict_data, clf_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5d7d1",
   "metadata": {
    "cell_id": "00025-2acf9c12-da85-4c1f-add5-f2b0f600177f",
    "deepnote_cell_height": 69.69999694824219,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Conclusiones Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996db4c2",
   "metadata": {
    "cell_id": "00026-15c07b20-0e16-48fa-bf3c-33aeb2c4c1db",
    "deepnote_cell_height": 51.53334045410156,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "En este capitulo, nos enfocamos en resolver el problema de clasifiacion para la prediccion de niveles de calificacion de peliculas. La metodologia utilizada consiste de los siguientes tres pasos: (1) Modelos de referencia, (2) Gridsearh y (3) Predicción.\n",
    "\n",
    "La etapa (1) consistio en diseñar dos modelos sencillos para el problema de clasifiacion. El primero de ellos es un modelo Dummy, que predice aleatoriamente la variable observada proporcional a numero de ocurrencias de cada etiqueta. El segundo modelo es un Baseline que resuelve la misma tarea pero esta vez sus predicciones dependen del input dado. Las metricas obtenidas por cada modelos son un F1-macro del 20% y 28%, resp.\n",
    "\n",
    "Ahora bien, dado estos modelos de referencias, la idea es encontrar un mejor modelo. La etapa (2) consiste en completar este objetivo. Para esto, se realiza una busqueda en grilla o gridsearch con distintos modelos de clasificacion, diferentes instancias de hiperparametros y representaciones de los datos. Preliminarmente se eligieron seis modelos de clasificación, de los cuales solo tres de ellos fueron utilizados para encontrar el mejor modelo. El resto de modelos fueron descartados ya que demoraba significativamente más su entrenamiento que el resto, o bien, por inspeccion sus metricas eran deficientes. Los tres modelos utilizados fueron DecisionTreeClassifier, RandomForestClassifier y LinearSVC. Finalmente, el resultado del gridsearch mostro que el ultimo de estos tres, era el mejor. Obteniendo un F1-macro del 36%. Desempeño mayor que ambos modelos de referencia.\n",
    "\n",
    "Finalmente, este mejor modelo fue utilizado para la prediccion del conjunto de prueba propuesto en la competencia. Etapa denominada como (3). Este mejor modelo obtenido en (2) obtuvo un F1-macro del X%. El cual es un valor ..... Este resultado puede deberse a que ....\n",
    "\n",
    "\n",
    "Para concluir, consideramos que resolver la tarea de clasificacion posee varios desafios subyacentes. Uno de ellos es que la variable a predecir es desbalanceda, por lo que los modelos encontrados tienden a predecir peor las clases minoritarias en comparacion a las clase que poseen más datos.\n",
    "\n",
    "Por otro lado, posibles mejoras que consideramos factibles para resolver este problema es probar otro tipo de modelo y un balance de datos. \n"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "dbddc0f3-10b8-4160-bc27-ac993196164c",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
